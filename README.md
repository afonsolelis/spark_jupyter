# üöÄ Aula Pr√°tica de Apache Spark com Jupyter

Este reposit√≥rio cont√©m uma aula completa e pr√°tica de Apache Spark usando dados reais de ve√≠culos el√©tricos do estado de Washington (EUA).

## üìã Conte√∫do da Aula

### üéØ Objetivos
- Comparar processamento **Python puro vs Apache Spark**
- Entender conceitos fundamentais do Spark (RDDs, DataFrames, lazy evaluation)
- Aprender opera√ß√µes essenciais: transforma√ß√µes, a√ß√µes, window functions
- Praticar **Spark SQL** para consultas complexas
- Implementar **otimiza√ß√µes** (cache, particionamento, broadcast joins)
- Realizar **an√°lise explorat√≥ria** de dados reais

### üìä Dataset
**Arquivo:** `electric_vehicles.json`
- **Fonte:** Washington State Department of Licensing
- **Conte√∫do:** Dados de ve√≠culos el√©tricos registrados (BEVs e PHEVs)
- **Campos principais:**
  - Marca, modelo, ano do ve√≠culo
  - Tipo (Battery Electric Vehicle ou Plug-in Hybrid)
  - Alcance el√©trico, pre√ßo MSRP
  - Localiza√ß√£o (cidade, condado, coordenadas)
  - Elegibilidade para incentivos

## üõ†Ô∏è Configura√ß√£o do Ambiente

### Pr√©-requisitos
- Python 3.12+
- Git

### 1. Clone o Reposit√≥rio
```bash
git clone <url-do-repositorio>
cd spark_jupyter
```

### 2. Instale as Depend√™ncias
```bash
pip install -r requirements.txt
```

**Principais bibliotecas instaladas:**
- `pyspark>=3.5.0` - Apache Spark para Python
- `pandas>=2.1.0` - Manipula√ß√£o de dados
- `numpy>=1.25.0` - Computa√ß√£o num√©rica
- `matplotlib>=3.8.0` - Visualiza√ß√µes
- `seaborn>=0.13.0` - Visualiza√ß√µes estat√≠sticas
- `jupyterlab>=4.0.0` - Ambiente interativo
- `pyarrow>=14.0.0` - Performance otimizada

### 3. Inicie o Jupyter Lab
```bash
jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
```

### 4. Abra o Notebook
Navegue at√© `spark_aula_veiculos_eletricos.ipynb` e execute as c√©lulas sequencialmente.

## üìö Estrutura da Aula

### 1. **Setup e Importa√ß√µes**
- Configura√ß√£o do ambiente
- Importa√ß√£o das bibliotecas necess√°rias

### 2. **Carregamento dos Dados**
- Leitura do arquivo JSON
- Estrutura√ß√£o dos dados
- An√°lise inicial da estrutura

### 3. **Processamento SEM Spark (Python Puro)**
- Implementa√ß√£o de an√°lises usando apenas Python nativo
- Medi√ß√£o de performance
- An√°lises inclu√≠das:
  - Contagem por marca
  - Distribui√ß√£o por tipo de ve√≠culo
  - Alcance el√©trico m√©dio
  - Crescimento temporal
  - Top cidades

### 4. **Setup do Spark**
- Cria√ß√£o da SparkSession
- Configura√ß√µes otimizadas
- Spark UI para monitoramento

### 5. **Processamento COM Spark**
- Cria√ß√£o de DataFrames
- Implementa√ß√£o das mesmas an√°lises usando Spark
- Compara√ß√£o de performance

### 6. **Compara√ß√£o de Performance**
- An√°lise dos tempos de execu√ß√£o
- Discuss√£o sobre quando usar cada abordagem
- Observa√ß√µes sobre escalabilidade

### 7. **An√°lise Explorat√≥ria dos Dados (EDA)**
- Estat√≠sticas gerais do dataset
- Verifica√ß√£o de dados nulos
- Distribui√ß√µes por diferentes dimens√µes

### 8. **Opera√ß√µes Avan√ßadas do Spark**
- **Transforma√ß√µes vs A√ß√µes** (lazy evaluation)
- **Window Functions** para an√°lises complexas
- **Spark SQL** para consultas declarativas
- Exemplos pr√°ticos de cada conceito

### 9. **Visualiza√ß√µes**
- Gr√°ficos comparativos usando matplotlib/seaborn
- An√°lise visual dos resultados
- Interpreta√ß√£o dos dados

### 10. **Otimiza√ß√µes e Melhores Pr√°ticas**
- Estrat√©gias de particionamento
- Cache e persist√™ncia
- Broadcast joins
- Configura√ß√µes importantes
- Predicate pushdown

### 11. **An√°lise de Performance e Monitoring**
- Spark UI para debugging
- Planos de execu√ß√£o
- Benchmarks de opera√ß√µes
- M√©tricas de performance

### 12. **Conclus√µes e Pr√≥ximos Passos**
- Resumo dos conceitos aprendidos
- Quando usar Spark vs alternativas
- Caminhos para aprofundamento

## üîç Principais Conceitos Demonstrados

### Spark Core
- **RDDs** (Resilient Distributed Datasets)
- **DataFrames** e **Datasets**
- **Transforma√ß√µes** (map, filter, groupBy, join)
- **A√ß√µes** (count, collect, show, save)
- **Lazy Evaluation** (otimiza√ß√£o autom√°tica)

### Spark SQL
- Consultas SQL declarativas
- Registrar DataFrames como tabelas tempor√°rias
- Fun√ß√µes agregadas e window functions
- Subconsultas e CTEs

### Otimiza√ß√µes
- **Cache/Persist** para reutiliza√ß√£o de DataFrames
- **Particionamento** por colunas estrat√©gicas
- **Broadcast Joins** para tabelas pequenas
- **Predicate Pushdown** autom√°tico
- **Adaptive Query Execution** (AQE)

## üìà An√°lises Realizadas

### An√°lises B√°sicas
- ‚úÖ Top 10 marcas de ve√≠culos el√©tricos
- ‚úÖ Distribui√ß√£o BEV vs PHEV
- ‚úÖ Alcance el√©trico m√©dio por marca
- ‚úÖ Crescimento de registros por ano
- ‚úÖ Top 10 cidades com mais ve√≠culos

### An√°lises Avan√ßadas
- ‚úÖ Ranking de marcas por ano (Window Functions)
- ‚úÖ An√°lise geogr√°fica por condado
- ‚úÖ Correla√ß√£o entre pre√ßo e alcance
- ‚úÖ Elegibilidade para incentivos
- ‚úÖ Distribui√ß√£o de concession√°rias el√©tricas

## üéØ Casos de Uso do Spark

### ‚úÖ Quando Usar Spark
- Datasets grandes (>1GB)
- Processamento distribu√≠do
- ETL complexo
- An√°lises em tempo real (Spark Streaming)
- Machine Learning em escala (MLlib)
- Processamento de dados em cluster

### ‚ùå Quando N√ÉO Usar Spark
- Datasets pequenos (<100MB)
- An√°lises simples
- Prot√≥tipos r√°pidos
- Recursos limitados
- Lat√™ncia ultra-baixa necess√°ria

## üîß Troubleshooting

### Problemas Comuns

**Erro de mem√≥ria:**
```bash
export PYSPARK_DRIVER_PYTHON_OPTS="--max-memory=4g"
```

**Spark UI n√£o carrega:**
- Verifique se a porta 4040 est√° dispon√≠vel
- Use `spark.sparkContext.uiWebUrl` para obter a URL

**Performance lenta:**
- Verifique o n√∫mero de parti√ß√µes com `df.rdd.getNumPartitions()`
- Use cache para DataFrames reutilizados: `df.cache()`
- Considere reparticionamento: `df.repartition(4, "coluna")`

**Erro de Java/Scala:**
- Verifique se JAVA_HOME est√° configurado
- Spark requer Java 8 ou 11

## üìñ Recursos Adicionais

### Documenta√ß√£o Oficial
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)

### Pr√≥ximos Passos
1. **Spark Streaming** - Processamento em tempo real
2. **MLlib** - Machine Learning distribu√≠do
3. **GraphX** - Processamento de grafos
4. **Delta Lake** - Versionamento de dados
5. **Databricks** - Plataforma Spark na nuvem

### Livros Recomendados
- "Learning Spark" - Holden Karau
- "Spark: The Definitive Guide" - Bill Chambers
- "High Performance Spark" - Holden Karau

## ü§ù Contribui√ß√£o

Sinta-se √† vontade para:
- Reportar bugs ou problemas
- Sugerir melhorias
- Adicionar novos exemplos
- Compartilhar casos de uso

## üìÑ Licen√ßa

Este projeto √© para fins educacionais. Os dados utilizados s√£o p√∫blicos e fornecidos pelo Washington State Department of Licensing.

---

## üéâ Divirta-se Aprendendo Spark!

Este tutorial foi criado para proporcionar uma experi√™ncia pr√°tica e completa com Apache Spark. Execute c√©lula por c√©lula, experimente modifica√ß√µes e explore os dados por conta pr√≥pria!

**Dica:** Use o Spark UI (normalmente em `http://localhost:4040`) para visualizar os jobs em execu√ß√£o e entender melhor como o Spark processa seus dados.